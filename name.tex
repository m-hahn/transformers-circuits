\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

% Use the lineno option to display guide line numbers if required.

\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}





\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}


\usepackage{amsthm}
 


\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
\newcommand{\key}[1]{\textbf{#1}}


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}
\newcommand{\rljf}[1]{{\color{blue}[rljf: #1]}}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}


\title{On the Hierarchical Capabilities of Self-Attention}
\author{Michael Hahn}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}
    Self-attention has been really successful in NLP.
    This poses the question of what kinds of structures they are capable of capturing.
    In particular hierarchy.
    We investigate the computational power of self-attention by showing equivalences between transformers and classes of Boolean circuits.
    
    We show theoretically and empirically that transformers can represent simple recursive patterns, and then  provide theoretical and experimental evidence that they are limited in their capability to model more complex hierarchical structures, such as evaluating the truth value of logical formulas.

We further show that capturing recursion crucially relies on the use of soft attention.

   Complements recent work on the power of transformers.
    We discuss implications of this for natural language and NLP.

%    First, we show that stacked argmax self-attention attention can only compute functions in $AC^0$, which entails strong limitations on the computations they can perform (compared to fully-connected networks).
%    Adding layer normalization, or using softmax self-attention, yields $TC^0$.
    
%    Limitations of transformers are believed to exist; this appears to be the first formal proof.
    
 
\end{abstract}


language hierarchical ...

transformers ...

we address this question by combining theory and experiments

our experiments use formal languages. we do this due to the close relation to the theory.

(1) simple recursion

(2) also simple recursion

(3) complex recursion: cannot work

(4) how does recursion play out in understanding language? would be cool to have something, like targeting bert on some examples??? (bert on center embedding or the tal linzen stuff would be cool)

DISCUSSION

(4) How do transformers and LSTM compare?
LSTM is more powerful in some ways (regular languages), less powerful in others (information flow)

(5) What aspects of self-attention are important?

- softness is very important

---------------------------------

comparison to LSTMs

- transformers (by conjecture) cannot do some regular languages. question for future work whether those languages ever turn up in NLP (whereas even SRNNs can in theory do all regular languages)

- can LSTMs really do Dyck well?

Q

- somehow do something with soft vs hard attention? at least mention

- 

- transformers have been tremendously successful in NLP

-- bert, gpt

- question what they (can) learn 

-- what are their limits?

-- how much of linguistic structure as linguists have identified can they encode?

-- how can we improve NLP?

- for more `traditional' recurrent models, some understanding

- some work on understanding transformers

- big problem for experimental work: hyperparameters

- we address this question:

- drawing on computational complexity

- will show limitations of transformers

-- with hard (argmax) attention, they are extremely limited: cannot do modulo counting, cannot do recursion

-- with softmax, they are more powerful, but -- if widely believed conjectures in computational complexity are true -- still cannot do more complex recursive tasks such as evaluate logical formulas

- show equivalence between transformers and certain classes of boolean circuits

- as a consequence, we transfer a host of results from computational complexity to transformers

- intuitively, transformers should not be able to do counting etc. We confirm this mathematically.

- experimentally

-- parity

-- recursion $a^nb^n$ with neutral letter

- We argue that our results have implications both for NLP and for linguistics.

- For NLP, our result confirms the idea that transformers cannot do recursion

- For linguistics, our results highlight an interesting question:
transformer-based models are the best quantitative models we have of language, but they are formally incapable of representing unbounded recursion -- reputedly a core property of language

Note that this is even stronger than with LSTMs:
LSTMs can do $a^nb^n$, transformers cannot (if there is a neutral letter).

- what to make of this?

-- language has recursion, but high depth is relatively infrequent, maybe due to human memory limitations



---------

It has been suggested that self-attention is computationally restricted, and (REF) proposed the universal transformer to address limitations.

No formal proof of the limitations, even though it seems intuitively plausible.

We show:
- Stacked self-Attention is strongly limited, coinciding with $FO[unary]$.

- Layer normalization and softmax self-attention both yield class coinciding with $TC^0[unary]$.
Widely believed to not be able to compute recursive things (such as evaluating logical formulas), BUT proving limitations on such architectures would be a breakthrough, so unlikelu.


\section{Introduce Transformers in more detail}

\section{Bounded-Depth Circuits}

FIGURE

In full generality, a Boolean circuit is a directed acyclic graph in which each vertex is labeled with some Boolean operation, and where the input and output vertices each are numbered.

Here we will mainly be concerned with circuits having $\wedge, \vee, \neg$, but one can construct circuits using other Boolean operations.

Clearly, circuits have a lot in common with feedforward networks, and the relation to quantized neural networks is particularly clear.
Also, a finite-precision feedforward net is a circuit.

Note a circuit -- like a feedforward net -- has fixed-dimensional input.

A circuit family assigns a circuit with $n$ inputs to each $n \in \mathbb{N}$.

A language is a set $L \subset \{0,1\}^\omega$.
A language is recognized by a circuit family iff ...

$AC^0$ is the class of languages recognized by circuit families that have (1) bounded depth, (2) polynomial size.





Parallel computation has been studied a lot

Circuit complexity

Some strong lower bounds.
In particular, FSS theorem. also some more things.
Before we show that these results

Our main technical result will be a reduction between (bounded precision) transformers and such $AC^0$ circuits.


Before we prove this, we discuss what this will give us.

Research has already identified lots of properties about the power of $AC^0$.

$AC^0$ circuits can deal with some simple properties

-- is there a 1 before a 0?

-- ...



The celebrated Furst-Saxe-Sipser theorem states that $AC^0$ circuits cannot recognize the \textsc{Parity} language, the language of bitstrings of even parity:
$$Parity = \{w \in \{0,1\}^n : n \in \mathbb{N}, parity(w) = 0\}$$
That is, in order to recognize this language, circuits using only and/or gates have to either grow in depth or become exponentially wide as $n$ increases.

As an example for the relevance of \textsc{Parity} for natural language:
Being able to solve \textsc{Parity} is a first prerequisite to evaluating logical formulas (due to iterated negation), and it is thus unsurprising that $AC^0$ cannot evaluate logical formulas.

In fact, $AC^0$ circuits cannot do recursion at all, beyond a trivial level:
They can do $a^nb^n$, but they cannot do the version of $a^nb^n$ that has a `neutral' letter sprinkled in.


make a picture of an $AC^0$ circuit



AC0 limitations:

- cannot decide whether the number of zeroes in a bitstring is even or odd

- more generally, the only regular languages solved are the `star-free' ones

- cannot handle recursion (cite tuebingen paper)

TC0 limitations:

- widely believed that it can't handle recursion

also relation to communication complexity

\section{Self-Attention with Argmax}

technicality:
For showing limitations, use Non-Uniform but bounded-precision transformers.
And we allow the dimensions to get bigger (polynomially) with $n$??? Only the number of layers has to be bounded. No I think this wouldn't fit into $AC^0$.
this way, corresponds exactly to $AC^0$.

Prop:
Assume bounded-precision arithmetic. Argmax attention, and NO layer normalization.
Input: Embeddings for discrete symbols (finite alphabet); positional embeddings are arbitrary (though only finitely many distinct ones due to precision arithmetic -- corresponds to the use of unary predicates).

Output: vector for each input word (say)

Then any such transformer can be simulated in $AC^0$. The transformer depth linearly relates to the circuit depth.

Consequence: cannot model modulo, nontrivial recursion things.


%If yo do have layer normalization, can do $TC^0$ things?!. There is only one majority gate per neuron and layer, really. So it's still linear-wire $TC^0$ -- the number of wires that feed into majority gates is still linear, while the number of wires feeding into and gates is quadratic.

Proof:

Such a computation can be done in $FO[<,unary]$.

\begin{proof}
we first note

- can do bounded-precision bounded-arity arithmetic.

- all local layer-wise computations can be done (feedforward, layer-norm, skip-connection)

- attention:

-- compute attention scores: ok

-- compute the max of the scores: can also be done, make explicit how it's done.

By induction, show we can compute the first $k$ significant bits of the max.

-- then do the argmax by matching. If there are multiple matches, we take the first one.

\end{proof}

Converse: Convert $FO[unary]$ to transformer



Converse

\section{Argmax? Softmax?}

Softmax gives all of $TC^0$ with suitable uniformity. But in NLP tends to be end up being argmax. if it's really top-k max will still be in $AC^0$.

show that SOTA MT system has attention distributions with low entropy

\section{Related work}

Limitations of transformers have been informally suggested, and experimentally supported.

- Tran et al

- Universal transformers

in a similar vein, power of GRUs: https://arxiv.org/pdf/1805.04908.pdf

\section{Experiments?!}

- show how transformers experimentally really can't do these things (e.g., scaling of required depth with length of input where it still works), and compare to LSTM.

- throughout compare with LSTM

- also compare on copy task: LSTM needs more units, transformer doesn't (?)

\subsection{Parity}
Extremely simple problem, a very simple regular language, recognized with a two-state automaton.
Here also theoretical predictions.

-- parity: show how model size vs length scale

-- attention: show how transformer does it, clearly cannot scale

(that this cannot work in $AC^0$: celebrated FSS theorem)

\subsection{Simple Recursion}
-- $a^nb^n$ with neutral letter

-- Dyck language

\subsection{Complex Recursion}

-- logical formulas

-- first order quantifier alternation

\section{finite precision}

- dropout also similar effect

- important problem for future research to understand this

\section{What does this mean for NLP?}

- language can be modeled really well -- in a quantitative/statistical sense -- with a model that cannot do recursion, or large classes of regular languages. does this mean recursion doesn't matter much in language? (at least, depth doesn't matter so much in naturally occurring language -- we knew that already, but it's an interesting counterpoint to arguments that we need hierarchical structure in NLP architectures...)

- does this mean transformers don't capture essential aspects of language?

% Ke Tran paper
% https://staff.science.uva.nl/c.monz/html/publications/D18-1503.pdf

% http://u.cs.biu.ac.il/~yogo/bert-syntax.pdf

% https://www.aclweb.org/anthology/D18-1458
%Rather than concluding that RNNs are superior toTransformers for the modeling of long-range de-pendency phenomena, we find that the number ofheads in multi-head attention affects the ability ofTransformers  to  model  long-range  dependenciesin subject-verb agreemen


% BIG QUESTION:
% What can you compute with infinite precision but noise (e.g. Gaussian dropout)? Not really finite-state? Compare Braverman et al paper.

\end{document}