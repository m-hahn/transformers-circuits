\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

% Use the lineno option to display guide line numbers if required.

\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}





\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{amssymb}


\usepackage{amsthm}
 


\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
\newcommand{\key}[1]{\textbf{#1}}


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}
\newcommand{\rljf}[1]{{\color{blue}[rljf: #1]}}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}


\title{On the Computational Capabilities of Self-Attention}
\author{Michael Hahn}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}
    Self-attention has been really successful in NLP.
    We investigate the computational power of self-attention.
    We show equivalences between transformers and classes of Boolean circuits, which entail strong limitations on the computational power of transformers.
    
    First, we show that stacked argmax self-attention attention can only compute functions in $AC^0$, which entails strong limitations on the computations they can perform (compared to fully-connected networks).
    Adding layer normalization, or using softmax self-attention, yields $TC^0$.
    
    Limitations of transformers are believed to exist; this appears to be the first formal proof.
    
    Complements recent work on the power of transformers.
    We discuss implications of this for natural language and NLP.
\end{abstract}


We consider the problem of language recognition: Transformer reads, and decides is the word in the language. Viewed as a seq2seq problem, with the target sequence being just TRUE or FALSE.



\section{Setup}

language recognition, essentially as in Weiss et al: Seq2Seq, transduce language to 0/1. We only demand that the result by separated from 0.5 by some constant $\delta$.


\section{Step 1 (Reduce first layer)}

For each $c$ and each head, we take the $c$ distinct inputs that can achieve the maximal values for a certain input.

First start with small $c$, and go up until $c$ is large enough for LLL.

Case 1: Can apply LLL.

$\left(\frac{1+p}{2}\right)^c \leq A B (1-A)^d$

$ \exp(-\frac{\delta^2(1-p)n}{(2+\delta)})  \leq B (1-A)^n$


\url{https://en.wikipedia.org/wiki/Chernoff_bound#Multiplicative_form_(relative_error)}

------------------

$\left(\frac{1+p}{2}\right)^c \leq A B (1-A)^{2c}$

$ \exp(-\frac{\delta^2(1-p)}{(2+\delta)})  \leq B^{1/n} (1-A)$


----------------------

$\left(\frac{1+p}{2}\right) \leq A^{1/c} B^{1/c} (1-A)^{2}$: e.g., for $c=20$, $A=0.01$, $B=0.9$, $p=0.5$

$ \exp(-\frac{\delta^2(1-p)}{(2+\delta)})  \leq B^{1/n} (1-A)$, e.g. for $\delta=0.9$, any $n \geq 1$

-------------------------

Case 2: $d$ has gotten too large, say $d > 2c$.

WLOG this holds for infinitely many $n$'s (otherwise we can just remove them, and return to Case 1). in any case, we remove all non-initial $n$'s for which this doesn't hold.
Then we take one relevant chain, and reduce throughout.

(If the first unsatisfied $n$ doesn't have a participating head, then we can apply LLL there and be done with it)

In the next step, $d$ can at most have decreased.

--------------------------

Case 3: $d$ has gotten infinite: Deal with it as in Case 2.

------------------------------

Assume in the end there is only bounded number of free inputs (we never got into Case 3, and we never got into Case 1).
So we must get into Case 2 infinitely often, so for $n \rightarrow \infty$, the number of times we have applied Case 2 must have been infinite.



\section{Step 2 (Reduce second layer)}

Let $e$ be the number of first-layer elements we want to consider for a given head.
That means, $ec$ input elements.

Let $f(e)$ be the number of neighboring second-layer heads. 

If $f(e)$ is infinite, or greater than $2ec$, then we're done again.

probability of bad event:
$1-((1-q)/2)^{c}$
obstacle here is that the first-layer elements are not independent, since they might use the same input bits.



\section{draft}

Iterate:
Take the smallest $n$ which hasn't been solved.

Case A: there is a head for which $d$ really is so large, then process an entire chain through all $n$'s (just by setting $c$ variables, we can satisfy $1.5 c$ many heads. Satisfying all the remaining heads can only take fixating at most $n-1.5c$ variables).

Case B: there is no so head, can just apply LLL

Assume that the number of free things is bounded, then we did only go through A finitely often, but then we must have applied LLL, contradiction.


\section{previous ideas}

STEP 1: Reduce the first layer. Apply random restrictions + LLL.

$p$ is the probability that a variable is left unassigned.

Bad events:

(1) $A_i$: did not hit any of the top $c$ inputs with the relevant input

(2) $A_0$ have more than $(1+\delta)(1-p)n$ assigned variables

$P(A_i) = (p+(1-p)/2)^c = \left(\frac{1+p}{2}\right)^c$

$P(A_0) \leq \exp(-\frac{\delta^2(1-p)n}{(2+\delta)})$

$A$ assigned the $A_i$ events, $B$ assigned to $A_0$.


$P(A_i) \leq A B (1-A)^n$

$P(A_0) \leq B (1-A)^n$

so we have

$\left(\frac{1+p}{2}\right)^c \leq A B (1-A)^n$

$ \exp(-\frac{\delta^2(1-p)n}{(2+\delta)})  \leq B (1-A)^n$

Set $A_n := \frac{1}{2^n}$:


$\left(\frac{1+p}{2}\right)^c \leq \frac{1}{2^n} B (1-\frac{1}{2^n})^n$ (doesn't work!!!)

$ \exp(-\frac{\delta^2(1-p)}{(2+\delta)})  \leq B^{1/n} (1-\frac{1}{2^n})$ (this one's okay)


-------------------------------------------------


STEP 1: Reduce the first layer. Apply random restrictions + LLL.

$p$ is the probability that a variable is left unassigned.

Bad events:

(1) $A_i$: did not hit any of the top $c$ inputs with the relevant input

(2) $A_0$ have more than $(1+\delta)(1-p)n$ assigned variables

$P(A_i) = (p+(1-p)/2)^c = \left(\frac{1+p}{2}\right)^c$

$P(A_0) \leq \exp(-\frac{\delta^2(1-p)n}{(2+\delta)})$

$A$ assigned the $A_i$ events, $B$ assigned to $A_0$.

TODO wrong!?!? each can be dependent on more than $c$ different ones

$P(A_i) \leq A B (1-A)^c$

$P(A_0) \leq B (1-A)^n$

so we have

$\left(\frac{1+p}{2}\right)^c \leq A B (1-A)^c$

$ \exp(-\frac{\delta^2(1-p)n}{(2+\delta)})  \leq B (1-A)^n$

So 

$\left(\frac{1+p}{2}\right) \leq A^{1/c} B^{1/c} (1-A)$ (so choose $A$ close to $0$, and $c$ big)

$ \exp(-\frac{\delta^2(1-p)}{(2+\delta)})  \leq B^{1/n} (1-A)$ (so choose $A$ close to $0$)

Conclusion: The mass of good restrictions is at least

$\geq (1-B)(1-A)^n$

-------------------------------------------------

STEP 2: Reduce the second layer

For each element in the second layer, the probability that we hit at least one of the top-$d$ inputs is at least $((1-q)/2)^c$.
So the probability that we did not hit any of the inputs is at most $\leq 1-((1-q)/2)^c$.

$1-((1-q)/2)^c \leq AB(1-A)^d$ -- so we need $q$ to be really small

$ \exp(-\frac{\delta^2(1-q)}{(2+\delta)})  \leq B^{1/n} (1-A)$ -- again, need $q$ to be small

TODO feels wrong, too easy.

--------------------------------------------------

Probability that none is colored the right way, for one head: $(p+(1-p)/2)^d$.

We have $4d P(bad) \leq 1$ for small $d$ (unless $p$ gets bigger), so by Lovasz Local Lemma, there is a good assignment. TODO but how do we ensure that this doesn't just end up restricting everything?

(2) Now we just have to repeat this on the next level.

Probability that none is colored the right way: heuristically $(c(p+(1-p)/2))^d$. but actually things aren't independent. TODO problem


\section{Infinite Precision Transformers and Regular Languages}

Self-attention itself is compatible with recognition of any language in the sense of separating them!? just positional embeddings are $2^{-n}$, sum them up, and have very complicated nonlinear transformation of a few numbers in the output.
More interesting question is what can be implemented with e.g. a ReLU MLP.

\begin{proposition}
We can do parity with perfect accuracy, but not with perfect cross-entropy (neither with SoftMax not with ArgMax). At least if the nonlinearities are Lipschitz (does LayerNorm respect Lipschitzness?).
\end{proposition}
%take PARITY. want to get a final prediction. 

%take two inputs where the bags of input vectors are extremely close. or flip one input in an extremely long word.

(Problem: not clear how the proof would work with HardMax attention. But SoftMax has its own problem with long words. One way to deal with this is to modify Softmax by introducing a threshold below which a logit becomes minus infinity. But then cannot enforce that there will be nonzero values)

\begin{proof}
%Take an extremely long word, of length $n$, with $\approx n/2$ ones and zeroes.

Take some very long $n_\epsilon$.

Take three positions $i, j, k \leq n$ such that their embeddings differ only by $\leq \epsilon$.

Now consider the word where there are zeros everywhere, except for $i$, or except for $i$ and $j$.

The first level will also only differ by $\leq C \epsilon$.

Now take attention weight of $i,j,k$ with some other $l$. They can differ only by $\leq C' \epsilon$.

Take softmax over attention weights, again attention weights differ only by $\leq C'' \epsilon$.

Also the summed up things will be almost the same. And so on.

TODO LayerNorm. (the SDs will also be almost the same)
\end{proof}

Contrast this with RNNs: they can do any regular language easily

In fact, transformers can't even do AND/OR well, since many small attention weights will ultimately drown out one big one. Cam change this to HardMax attention.

QUESTION: Regular languages computable by HardMaxAtttention: $FO_2[<]$ ? HardMaxAttention can do all languages in this. Can it do additional ones?

Equations of $DA$: $(xyz)^\omega y (xyz)^\omega = (xyz)^\omega$

Also DA: $Me^\omega M \subseteq MsM$ imply $e^\omega s e^\omega = e^\omega$



How about $(ab)^*$? cannot do this either (this is not in $FO_2[<]$)

\section{Experiment: Transformers and Parity, requires large weights}

\section{Finite Precision, Argmax}

Finite precision transformers are related to Boolean circuits with bounded depth, linear number of gates, and very strong uniformity.

\begin{proposition}
Can at most do languages of this and that type (not restricted to regular, but extremely restricted).
\end{proposition}

- finite precision and argmax: AC0 with unary predicates and linear gates

\section{Finite Pecision, Softmax}

- finite precision and softmax: Majority with two variables and unary predicates

actually extremely restricted

\section{infinite precision and argmax}: ? TODO?

- infinite precision and softmax (can use a rational approximation to softmax!?) (certainly cannot do good in the sense of cross-entropy, though perhaps hard to prove for arbitrary positional encodings. accuracy more challenging)

-- what can we say about the predicate class?

-- something about cross-entropy:

conjecture Furst-Saxe-Sipser like result


----------------------

result

- transformers can do some basic recursion: $a^nb^n$ with neutral letter, single-Dyck (can use attention map to illustrate how it's done)

they can in the sense of argmax-accuracy

but can they also do it with optimal cross-entropy? (modulo a bound on the weights) (e.g., in the sense of perfectly saying whether the word is valid or not, for some length distribution with infintie support)
a thresholding thing cannot really do this. so less powerful than an LSTM!?

- formally, transformers sorta coincide wtih $LTC^0$

-- presumably cannot do multi-Dyck!? (or even multi $a^nb^n$ with neutral letter) (can argue that LSTMs cannot do this either)

(BUT if the goal is just to say, assuming wellformedness, predict the one next thing, then it works. this is different from recognizing though, which would rquire quadratically many hedas.)

- neither can do logical formulas (assuming widely believed circuit lower bounds)

- on the other hand, there are regular languages that transformers cannot do (assuming those lower bounds)

- we also show that transformer's ability to do any kind of recursion crucialy relies on the use of soft -- as opposed to hard -- attention.
indeed, without soft attention, can do neither $a^nb^n$ not parity

- is there something where transformers are better than LSTMs?

(-- boring: things that can be solved by looking at positions, such as $a^nb^n$)

-- without counting or infinite precision, LSTM memory scales with number of units. for transformers, bottleneck is instead the ease of retrieving and combining info.

-- example? high crypticity process where everything MIGHT become relevant, but one doesn't know whether it will. e.g., $X_t$ informs which aspect of the past is relevant for $X_{t+1}$. as long as one can then also retrieve this part from the past, this is useful. maybe can do some actual language modeling thing where this matters. e.g. carrying around a lot of names.



transformer cannot do anbn version with two letters
\begin{proof}
we formalize transformer with order + unary
\end{proof}


on the other hand, actually LSTMs can use fractal encoding of stack

transformer cannot do such a thing. specifically, cannot do two-bracket neutral letter anbn. for simplicity can do incremental prediction. can you flesh out this argument?



LSTMs cannot do the anbn version with two letters
\begin{proof}
We work in the finite precision setting assumed by Weiss Goldberg ... 2018.

Note that at finite precision $h_t$ assumes finitely many values.

$c_t$ is a number (integer + remainder from a finite set). At each step, multiplied by $f_t$, and a finite value in $[-1,+1]$ is added.
If this multiplier $f_t$ is not saturated unboundedly often for some dimension of $c$, then this would destroy information.
Consequence: LSTM can only really do finite state + counting + multiplication with one of these numbers.

also control is saturated if c is really large, so always have to have some small c that reflects what to do next. but 
\end{proof}


issue: fixed transformer cannot simulate hard max over unboundedly long sequences, need to get larger and larger parameters to still enforce.

transformer cannot do anbn+neutral with perfect perplexity
\begin{proof}
at least heuristically, a single thing can't contribute much
\end{proof}



-----------------------------------

The success of transformers poses the question of what kinds of things they're capable of representing.
There is quite a range of studies on LSTMs/GRUs/SRNNs from this perspective, but nothing on transformers.
We examine the power of self-attention from a formal languages point of view, and compare to LSTMs.

(1) transformers can do basic recursion, and this ability crucially relies on the presence of soft (as opposed to hard) attention. in fact, with hard attention, they couldn't even count modulo 2. also, their ability to do this is a bit restricted compared to LSTMs (perplexity has to worsen with length)

(2) as it gets more complicated, neither transformers nor LSTMs work -- at least in the finite precision setting (Weiss et al. setting).
In particular, neither can do logical formulas (generally, neither of them can do a stack).
We show this by a combination of experiment, proof for LSTM, reduction to widely believed conjecture for transformer.

(3) there are regular languages that transformers cannot do (assuming this conjecture).

(4) we exhibit languages/processes that transformers do better on than LSTMs.



------------

- transformers have been tremendously successful in NLP

-- bert, gpt

- question what they (can) learn 

-- what are their limits?

-- how much of linguistic structure as linguists have identified can they encode?

-- how can we improve NLP?

- for more `traditional' recurrent models, some understanding

- some work on understanding transformers

- big problem for experimental work: hyperparameters

- we address this question:

- drawing on computational complexity

- will show limitations of transformers

-- with hard (argmax) attention, they are extremely limited: cannot do modulo counting, cannot do recursion

-- with softmax, they are more powerful, but -- if widely believed conjectures in computational complexity are true -- still cannot do more complex recursive tasks such as evaluate logical formulas

- show equivalence between transformers and certain classes of boolean circuits

- as a consequence, we transfer a host of results from computational complexity to transformers

- intuitively, transformers should not be able to do counting etc. We confirm this mathematically.

- experimentally

-- parity

-- recursion $a^nb^n$ with neutral letter

- We argue that our results have implications both for NLP and for linguistics.

- For NLP, our result confirms the idea that transformers cannot do recursion

- For linguistics, our results highlight an interesting question:
transformer-based models are the best quantitative models we have of language, but they are formally incapable of representing unbounded recursion -- reputedly a core property of language

Note that this is even stronger than with LSTMs:
LSTMs can do $a^nb^n$, transformers cannot (if there is a neutral letter).

- what to make of this?

-- language has recursion, but high depth is relatively infrequent, maybe due to human memory limitations



---------

It has been suggested that self-attention is computationally restricted, and (REF) proposed the universal transformer to address limitations.

No formal proof of the limitations, even though it seems intuitively plausible.

We show:
- Stacked self-Attention is strongly limited, coinciding with $FO[unary]$.

- Layer normalization and softmax self-attention both yield class coinciding with $TC^0[unary]$.
Widely believed to not be able to compute recursive things (such as evaluating logical formulas), BUT proving limitations on such architectures would be a breakthrough, so unlikelu.


\section{Introduce Transformers in more detail}

\section{Bounded-Depth Circuits}

FIGURE

In full generality, a Boolean circuit is a directed acyclic graph in which each vertex is labeled with some Boolean operation, and where the input and output vertices each are numbered.

Here we will mainly be concerned with circuits having $\wedge, \vee, \neg$, but one can construct circuits using other Boolean operations.

Clearly, circuits have a lot in common with feedforward networks, and the relation to quantized neural networks is particularly clear.
Also, a finite-precision feedforward net is a circuit.

Note a circuit -- like a feedforward net -- has fixed-dimensional input.

A circuit family assigns a circuit with $n$ inputs to each $n \in \mathbb{N}$.

A language is a set $L \subset \{0,1\}^\omega$.
A language is recognized by a circuit family iff ...

$AC^0$ is the class of languages recognized by circuit families that have (1) bounded depth, (2) polynomial size.





Parallel computation has been studied a lot

Circuit complexity

Some strong lower bounds.
In particular, FSS theorem. also some more things.
Before we show that these results

Our main technical result will be a reduction between (bounded precision) transformers and such $AC^0$ circuits.


Before we prove this, we discuss what this will give us.

Research has already identified lots of properties about the power of $AC^0$.

$AC^0$ circuits can deal with some simple properties

-- is there a 1 before a 0?

-- ...



The celebrated Furst-Saxe-Sipser theorem states that $AC^0$ circuits cannot recognize the \textsc{Parity} language, the language of bitstrings of even parity:
$$Parity = \{w \in \{0,1\}^n : n \in \mathbb{N}, parity(w) = 0\}$$
That is, in order to recognize this language, circuits using only and/or gates have to either grow in depth or become exponentially wide as $n$ increases.

As an example for the relevance of \textsc{Parity} for natural language:
Being able to solve \textsc{Parity} is a first prerequisite to evaluating logical formulas (due to iterated negation), and it is thus unsurprising that $AC^0$ cannot evaluate logical formulas.

In fact, $AC^0$ circuits cannot do recursion at all, beyond a trivial level:
They can do $a^nb^n$, but they cannot do the version of $a^nb^n$ that has a `neutral' letter sprinkled in.


make a picture of an $AC^0$ circuit



AC0 limitations:

- cannot decide whether the number of zeroes in a bitstring is even or odd

- more generally, the only regular languages solved are the `star-free' ones

- cannot handle recursion (cite tuebingen paper)

TC0 limitations:

- widely believed that it can't handle recursion

also relation to communication complexity

\section{Self-Attention with Argmax}

technicality:
For showing limitations, use Non-Uniform but bounded-precision transformers. this way, corresponds exactly to $AC^0$.

Prop:
Assume bounded-precision arithmetic. Argmax attention, and NO layer normalization.
Input: Embeddings for discrete symbols (finite alphabet); positional embeddings are arbitrary (though only finitely many distinct ones due to precision arithmetic -- corresponds to the use of unary predicates).

Output: vector for each input word (say)

Then any such transformer can be simulated in $AC^0$. The transformer depth linearly relates to the circuit depth.

Consequence: cannot model modulo, nontrivial recursion things.


%If yo do have layer normalization, can do $TC^0$ things?!. There is only one majority gate per neuron and layer, really. So it's still linear-wire $TC^0$ -- the number of wires that feed into majority gates is still linear, while the number of wires feeding into and gates is quadratic.

Proof:

Such a computation can be done in $FO[<,unary]$.

\begin{proof}
we first note

- can do bounded-precision bounded-arity arithmetic.

- all local layer-wise computations can be done (feedforward, layer-norm, skip-connection)

- attention:

-- compute attention scores: ok

-- compute the max of the scores: can also be done, make explicit how it's done.

By induction, show we can compute the first $k$ significant bits of the max.

-- then do the argmax by matching. If there are multiple matches, we take the first one.

\end{proof}

Converse: Convert $FO[unary]$ to transformer



Converse

\section{Argmax? Softmax?}

Softmax gives all of $TC^0$ with suitable uniformity. But in NLP tends to be end up being argmax. if it's really top-k max will still be in $AC^0$.

show that SOTA MT system has attention distributions with low entropy

\section{Related work}

Limitations of transformers have been informally suggested, and experimentally supported.

- Tran et al

- Universal transformers

in a similar vein, power of GRUs: https://arxiv.org/pdf/1805.04908.pdf

\section{Experiments?!}

- show how transformers experimentally really can't do these things (e.g., scaling of required depth with length of input where it still works), and compare to LSTM.

- throughout compare with LSTM

\subsection{Parity}
Extremely simple problem, a very simple regular language, recognized with a two-state automaton.
Here also theoretical predictions.

-- parity: show how model size vs length scale

-- attention: show how transformer does it, clearly cannot scale

(that this cannot work in $AC^0$: celebrated FSS theorem)

\subsection{Simple Recursion}
-- $a^nb^n$ with neutral letter

(that this cannot work in $AC^0$ can be seen from an elementary communication complexity argument)

-- Dyck language

-- logical formulas

-- first order quantifier alternation

\section{finite precision}

- dropout also similar effect

- important problem for future research to understand this


\section{LSTMs cannot really do Dyck with more than one symbol}

We work in the finite precision setting assumed by Weiss Goldberg ... 2018.

Note that at finite precision $h_t$ assumes finitely many values.

$c_t$ is a number (integer + remainder from a finite set). At each step, multiplied by $f_t$, and a finite value in $[-1,+1]$ is added.
If this multiplier $f_t$ is not saturated unboundedly often, then this would destroy information.
Consequence: LSTM can only really do finite state + counting.


\section{What does this mean for NLP?}

- language can be modeled really well -- in a quantitative/statistical sense -- with a model that cannot do recursion, or large classes of regular languages. does this mean recursion doesn't matter much in language? (at least, depth doesn't matter so much in naturally occurring language -- we knew that already, but it's an interesting counterpoint to arguments that we need hierarchical structure in NLP architectures...)

- does this mean transformers don't capture essential aspects of language?

% Ke Tran paper
% https://staff.science.uva.nl/c.monz/html/publications/D18-1503.pdf

% http://u.cs.biu.ac.il/~yogo/bert-syntax.pdf

% https://www.aclweb.org/anthology/D18-1458
%Rather than concluding that RNNs are superior toTransformers for the modeling of long-range de-pendency phenomena, we find that the number ofheads in multi-head attention affects the ability ofTransformers  to  model  long-range  dependenciesin subject-verb agreemen


% BIG QUESTION:
% What can you compute with infinite precision but noise (e.g. Gaussian dropout)? Not really finite-state? Compare Braverman et al paper.

\end{document}
