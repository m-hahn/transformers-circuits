\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}




\renewcommand{\baselinestretch}{0.989}



%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

% Use the lineno option to display guide line numbers if required.

\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


\newcommand{\Prob}{\mathbb{P}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}





\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{amssymb}


\usepackage{amsthm}
 

\allowdisplaybreaks

\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
\newcommand{\key}[1]{\textbf{#1}}


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\response[1]{{\color{blue}#1}}


\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}

\usepackage{fullpage}


\title{Response to Editor and Reviewer Comments}
\date{\today}
\begin{document}
\maketitle 

\response{We thank the reviewers and the editor for their feedback.
We are also grateful for the decision to grant us two additional pages, which we used to expand and clarify the mathematical presentation, and add further discussion of the relevance of our results.}

\section{Editor's Comments}

(a) Make more explicit the connection between your presentation to the one
in the original Vaswani et al transformer paper (queries, keys and values),
or otherwise refer to other common and somewhat standardized presentations
of the transformer model that do not use this terminology.

\response{We have added explicit pointers to queries, keys, and values in Section 3: First, we added the following to the paragraph following Equation (1): TODO}

\response{Second, we added the following behind Equation (2): TODO}

(b) Add more salient discussion of skip-connection / residual connections,
and in particular remark on them in the proofs. While reviewer C believes
the results should hold also with residual connections, I initially shared
reviewer A's reaction of not understanding this detail, and I am fairly
certain other readers will also face similar confusion. Please enhance the
relevant proofs with discussion of why they work also in the presence of
residual connections.

\response{We have added pointers to residual connections in the following places:}

\response{After Equation (3):}

\response{Figures 1 and 2: We added dashed connections marking skip connections, and added the following to the caption TODO}

\response{Last paragraph of Section 5: TODO}

\response{Section 6: TODO}

(c) Expand the discussion on the relevancy of the result. In particular:
- (c1) Expand on your thoughts on what the value of showing such an
asymptotic property is (traditionally, proofs of unexpressability in the
limit are really stand-ins for proofs of ungeneralizability).

\response{TODO}

- (c2) In particular, expand on the applicability of the results to
processing of natural language data and what is the takeaway to NLP
practitioners, if any. Empirical evidence suggests that the transformer
architecture is indeed preferable to other architectures in capturing longer
distance context, how is this reconciled with the theoretical findings?
- Note that it is perfectly OK to have a theoretical result that does not
have any relevance to empirical NLP work, and I find the result to be
interesting enough to be published also without such relevance. But, if that
is indeed the case, it needs to be stated explicitly.

\response{TODO}

(d) Attempt to Improve / simplify / expand the presentation of mathematics
used in sections 5 and 6 to cater to audiences who are less familiar with
these literature and techniques.

\response{We have used the additional space to expand various parts of Sections 5 and 6:}

\response{(1) We added Figure 3, which explains how the sequence $i^{(z)}_1, \dots, i^{(z)}_k$ is constructed, and added a pointer to this figure in the paragraph below Equation (9). While none of the reviewers asked specifically about this part, we felt this was worth adding, because the definition of this sequence is the crucial starting point for the entire proof (e.g., the notion of `k-dependendence' is based on it), and colleagues found that the prose description alone was a bit dense.}

\response{(2) TODO}

\response{... We have expanded the proof of Theorem 7. Reviewer B had noted that it was too brief, so we now explain in more detail the argument for \textsc{Parity} and \textsc{2Dyck}.}

%\response{Additionally, we have expanded the proof to give an explicit bound on the input length starting at which our results provide limitations in the hard attention case. Reviewer C had noted that our original proof provided an exponential tower upper bound; we now show that a much gentler growth can be shown. This increased the complexity of the proof only minimally, and -- more importantly -- we feel that this provides a more informative answer to the question of practical relevance.}

\section{Reviewer A:}

(Minor issue): I am confused by the discussion of languages that can and
can't be
shown to be modeled by transformers in lines 420-487. It is said "A
crucial difference between these languages and PARITY / 2DYCK is that
fixing a few inputs can easily force nonmembership, e.g. a single 0
for 1*, and an a in the second half for $a^n b^n$." Couldn't you use the
same argument to restrict the first element to ']' and thereby force
nonmembership in 1DYCK (and thus 2DYCK)? I do buy this
argument for PARITY; clearly you can set C such that even when forcing
Cn input symbols to any particular value, the status of the string is
unknown. If I understand the general argument as applied to PARITY,
then you could force 1-Cn bits to be all 0. If done, then for all
sequences longer than 2*(1-Cn+c) PARITY is unrecognizable; you could
at least test non-membership for shorter sequences (though still not
membership).

\response{We acknowledge that our phrasing was insufficiently specific. It is true that, as Reviewer A remarks, nonmembership can be enforced by fixing a few $^*$specific$^*$ inputs also for 1Dyck and certain other languages that are not recognized by transformers. The difference between, e.g., 1Dyck and $a^nb^n$ is that, for $a^nb^n$, $^*$any$^*$ part of the input is suitable (positions in the first half can be fixed to $b$, positions in the second half to $a$), whereas only positions at the boundaries are suitable when applying this to 1Dyck. We have clarified this by adding the phrase TODO}


The Transformer paper itself (section 3.1) critically notes the
inclusion of a so-called residual connection
at each layer that ensures an aspect of each input is propagated up the
layer
hierarchy, independent of the attention mechanism. This means that
each unit will be dependent on the corresponding unit (and ultimately
input symbol) of the layer below it. The restriction selection
mechanism does not take this into account. While it seems appropriate to
abstract away specifics of attention design, representation, and even
to treat soft attention as hard, the residual connection is a critical part
of the model and cannot be ignored. That there is a required
influencer for each node and that these cover ultimately the entire
input sequence would seem to undermine the core argument of this paper.

\response{Residual conections do not cause a problem for the proof because the final prediction is determined by $y_n^{(L)}$, which -- after applying the depth reduction method -- will only have a skip connection from $y_n^{(0)}$, which depends on only finitely many inputs.}

\response{We have added pointers to residual connections in various points of the paper, as explained in the response to the editor. }


Setting aside the possible flaw noted above, it is important to ask, is
there a practical effect of the results in this paper? The
restrictions that accomplish the described insensitivity behavior are
not arbitrary, but are in fact quite carefully crafted. This makes the
result in some sense akin to the adversarial image recognition
findings, which showed brittle recognition properties such that an
imperceptible (to human) change in an image could lead to
catastrophic recognition results. But that brittleness is based on
training methodology, not formalism, and transformers, as well as
RNNs, CNNs, and even pre-neural models (and pre-statistical models)
are sucseptible to that or similar brittleness. The formalism
brittleness would be much harder to trigger in practice (since access
to the typically unexposed weights is necessary, in order to find
maximal attention units), so the threat
of adversaries is not increased. The degree to which Transformer's
(formal) limits clash with human language expressivity does not seem
to be, in practice, more severe than with other formalisms; indeed, it
seems empirically to be less severe. The fundamental problem of full
human language modeling remains in any case; it requires wide ranging,
sporadic, multimodal, though ultimately finite context to capture all
of common sense. Transformer *does* appear to capture more,
longer-distance context and can in practice be trained to do so better
than other models.

\response{As described in more detail in the response to the editor, we have expanded the Discussion section, including discussion of the question of practical relevance.}

\section{Reviewer B:}


A formal proof to replace such intuitions would be progress. But my
confusion about the formalization already starts on page 3, where I can’t
square the definitions of (hard and soft) attention heads with those in
Vaswani et al 2017. Where are the value, key and query components that are
so crucial in that work?

\response{As detailed in the response to the editor, we have added explicit references to values, keys, and queries.}

The confusion continues throughout section 5, where first ‘restrictions’
are introduced. The basic idea, the paper says, is that a small fraction of
the input is fixed in a particular way. Perhaps this is a round-about way to
define a subset of the Parity/2Dyck languages for which the Tranformer’s
failure will be demonstrated. Restrictions are then said to be ‘applied to
the transformer’. That is also counterintuitive; I would expect
restrictions on models in proofs of what model can do (even when
restricted), and not in proofs on what they cannot do. It is probably my
lack of knowledge about the Boolean circuit literature, but I would need
more explanation and perhaps terminology more carefully tuned to an
ACL*-audience here to follow this entire section.

\response{We have, at least for the definition, added the more informative term `input restriction' to clarify that inputs are being restricted, not models. We have also added an explicit reference to the term `restriction' in the captions of Figures 1 and 2 (TODO).}

I don’t fare much better in section 6, where a different set of
mathematical techniques is used to prove theorem 6 and lemma 5. The paper
uses a simple PCFG to define the input language, but I am immediately at a
loss when the proofs start after giving theorem 6. What is an *equally
likely* non-member that yields similar but different output activations?

\response{We agree that the proof of Theorem 6 was very dense, we have substantially expanded it to clarify the argument.}

*Relevance*
Even if all the proofs are correct, I have some doubts about how relevant
all this is for understanding the behavior of the Transformer on real
language data. The paper, on page 10, briefly discusses the very reasonable
objection that the Transformer circumvents limitations of the sort studied
in this paper by using a large number of layers and attention heads, and
that natural language data simply doesn’t contain those deeply nested
dependencies, because humans fail at them too. This is such a crucial issue
for assessing the relevance of the presented proofs, that I think it would
deserve some longer discussion and some supportive empirical data.

\response{As detailed in the response to the editor, we have expanded the Discussion section to address the question of relevance.}


\section{Reviewer C:}

The most significant drawback I see in this work is that its results are
asymptotic, i.e. apply only to input sequences that are very long.  This
means that from a practical perspective, it is unclear if the limitations
established are at all relevant.  The authors acknowledge this fact and
discuss it explicitly in Section 7.  Nonetheless, I would suggest being a
bit more transparent in terms of the input lengths needed for the results to
hold.  As far as I could tell, there is an exponential and even a tetration
("exponential tower") blow-up.  The second main comment I have is that it
seems to me like the results for soft attention can actually be applied to
any stateless model processing arbitrarily long inputs with a fixed number
of parameters.  This means that they do not shed light on the specific
nature of transformers (self attention).  Again, the authors mention this
but I believe it could be stressed a bit more boldly (e.g. in the discussion
in Section 7).  Overall, the two shortcomings I raised are inherent to this
work, i.e. cannot be addressed without major changes, and since they are
acknowledged in the text, I do not think they should be treated as an
impediment for publication.

\response{As detailed in the response to the editor, we have expanded the Disucssion section to address the question of practical relevance. Regarding the soft attention results, we have added a few sentences about what kinds of architectures the proof is aplicable to (TODO).}

\begin{verbatim}
Minor comments:
* Typo in line 151: agreeement
* Typo in line 195: reccurent
* Typo in line 355: y^k_(j) (parenthesis should be in superscript)
* In Theorem 1, it should be stated that the result applies to hard
attention.
* In Equation (4), there is a typo with n being a subscript where it should
be a superscript
* Typo in line 850: some some
\end{verbatim}

\response{Thanks for pointing these out, we fixed these.}

\end{document}