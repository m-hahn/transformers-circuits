@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2017},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge},
  edition={2nd}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}


@article{merrill2019sequential,
  title={Sequential neural networks as automata},
  author={Merrill, William},
  journal={arXiv preprint arXiv:1906.01615},
  year={2019}
}



@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}

@inproceedings{
perez2019turing,
title={On the {Turing} Completeness of Modern Neural Network Architectures},
author={Jorge Pérez and Javier Marinković and Pablo Barceló},
booktitle={International Conference on Learning Representations},
year={2019}
}


@nproceedings{chen2017recurrent,
    title = "Recurrent Neural Networks as Weighted Language Recognizers",
    author = "Chen, Yining  and
      Gilroy, Sorcha  and
      Maletti, Andreas  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1205",
    doi = "10.18653/v1/N18-1205",
    pages = "2261--2271",
    abstract = "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.",
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
  pages={132--150}
}



@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}


@inproceedings{dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019}
}}

@inproceedings{tran2018importance,
    title = "The Importance of Being Recurrent for Modeling Hierarchical Structure",
    author = "Tran, Ke  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "4731--4736",
    abstract = "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures{---}recurrent versus non-recurrent{---}with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at \url{https://github.com/ ketranm/fan_vs_rnn}",
}

@inproceedings{yang2019assessing,
  title={Assessing the Ability of Self-Attention Networks to Learn Word Order},
  author={Yang, Baosong and Wang, Longyue and Wong, Derek F and Chao, Lidia S and Tu, Zhaopeng},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2019}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (SFCS 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10}}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in {AC}0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on {SRN} behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@inproceedings{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  booktitle={IJCAI'18 Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  year={2018},
  pages={4345--4352}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for rnn/cnn-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@inproceedings{chen2018best,
    title = "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
    author = "Chen, Mia Xu  and
      Firat, Orhan  and
      Bapna, Ankur  and
      Johnson, Melvin  and
      Macherey, Wolfgang  and
      Foster, George  and
      Jones, Llion  and
      Schuster, Mike  and
      Shazeer, Noam  and
      Parmar, Niki  and
      Vaswani, Ashish  and
      Uszkoreit, Jakob  and
      Kaiser, Lukasz  and
      Chen, Zhifeng  and
      Wu, Yonghui  and
      Hughes, Macduff",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    pages = "76--86",
    abstract = "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT{'}14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
}



@inproceedings{hao2019modeling,
    title = "Modeling Recurrence for Transformer",
    author = "Hao, Jie  and
      Wang, Xing  and
      Yang, Baosong  and
      Wang, Longyue  and
      Zhang, Jinfeng  and
      Tu, Zhaopeng",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1198--1207",
    abstract = "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence modeling hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.",
}

@inproceedings{
paulus2017deep,
title={A Deep Reinforced Model for Abstractive Summarization},
author={Romain Paulus and Caiming Xiong and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018}
}


@inproceedings{cheng2016long,
    title = "Long Short-Term Memory-Networks for Machine Reading",
    author = "Cheng, Jianpeng  and
      Dong, Li  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    pages = "551--561",
}


@inproceedings{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2017}
}


@inproceedings{parikh2016decomposable,
	    title = "A Decomposable Attention Model for Natural Language Inference",
	    author = {Parikh, Ankur  and
	      T{\"a}ckstr{\"o}m, Oscar  and
	      Das, Dipanjan  and
	      Uszkoreit, Jakob},
	    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
	    month = nov,
	    year = "2016",
	    address = "Austin, Texas",
	    publisher = "Association for Computational Linguistics",
	    pages = "2249--2255",
	}

@inproeedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}


@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? {A}n Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@article{dai2019transformer,
  title={Transformer-{XL}: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Cohen, William W and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{gulordava2018colorless,
    title = "Colorless Green Recurrent Networks Dream Hierarchically",
    author = "Gulordava, Kristina  and
      Bojanowski, Piotr  and
      Grave, Edouard  and
      Linzen, Tal  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    pages = "1195--1205"
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={LSTM recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, E},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary {E}nglish},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive science},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}


@article{lin2019open,
  title={Open {S}esame: Getting Inside {BERT}'s Linguistic Knowledge},
  author={Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  journal={arXiv preprint arXiv:1906.01698},
  year={2019}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{tenney2019bert,
  title={{BERT} rediscovers the classical {NLP} pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={57th Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}


@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
        year = {1963}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={Computational Complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{
miller2018recurrent,
title={Stable Recurrent Models},
author={John Miller and Moritz Hardt},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}


@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@article{parker2017cue,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  journal={Language processing and disorders},
  pages={121--144},
  year={2017},
  publisher={Cambridge Scholars Publishing Newcastle}
}




@article{barrington1992regular,
  title={Regular languages in {NC}1},
  author={Barrington, David A Mix and Compton, Kevin and Straubing, Howard and Th{\'e}rien, Denis},
  journal={Journal of Computer and System Sciences},
  volume={44},
  number={3},
  pages={478--499},
  year={1992},
  publisher={Elsevier}
}

@article{korsky2019computational,
  title={On the Computational Power of {RNN}s},
  author={Samuel A. Korsky and Robert C. Berwick},
  journal={arXiv preprint arXiv:1906.06349},
  year={2019}
}

@proceedings{hsieh2019robustness,
title	= {On the Robustness of Self-Attentive Models},
editor	= {Yu-Lun Hsieh and Minhao Cheng and Da-Cheng Juan and Wei Wei and Wen-Lian Hsu and Cho-Jui Hsieh},
year	= {2019},
booktitle	= {Annual Meeting of the Association for Computational Linguistics (ACL)}
}

@inproceedings{levy2018long,
  title={Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum},
  author={Levy, Omer and Lee, Kenton and FitzGerald, Nicholas and Zettlemoyer, Luke},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={732--739},
  year={2018}
}

@article{ostmeyer2019machine,
  title={Machine learning on sequential data using a recurrent weighted average},
  author={Ostmeyer, Jared and Cowell, Lindsay},
  journal={Neurocomputing},
  volume={331},
  pages={281--288},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{lei2017deriving,
  title={Deriving neural architectures from sequence and graph kernels},
  author={Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2024--2033},
  year={2017},
  organization={JMLR. org}
}
@article{yin2017comparative,
  title={Comparative Study of CNN and RNN for Natural Language Processing},
  author={Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1702.01923},
  year={2017}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={933--941},
  year={2017},
  organization={JMLR. org}
}
@article{bradbury2016quasi,
  title={Quasi-recurrent neural networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}
@article{lei2015semi,
  title={Semi-supervised question retrieval with gated convolutions},
  author={Lei, Tao and Joshi, Hrishikesh and Barzilay, Regina and Jaakkola, Tommi and Tymoshenko, Katerina and Moschitti, Alessandro and Marquez, Lluis},
  journal={arXiv preprint arXiv:1512.05726},
  year={2015}
}
@inproceedings{sundermeyer2013comparison,
  title={Comparison of feedforward and recurrent neural network language models},
  author={Sundermeyer, Martin and Oparin, Ilya and Gauvain, J-L and Freiberg, Ben and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8430--8434},
  year={2013},
  organization={IEEE}
}
@inproceedings{balduzzi2016strongly,
  title={Strongly-Typed Recurrent Neural Networks},
  author={Balduzzi, David and Ghifary, Muhammad},
  booktitle={International Conference on Machine Learning},
  pages={1292--1300},
  year={2016}
}


@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@book{chomsky1957syntactic,
  title={Syntactic structures.},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton},
  address={The Hague}
}


@article{keener1992limit,
  title={Limit theorems for random walks conditioned to stay positive},
  author={Keener, Robert W and others},
  journal={The Annals of Probability},
  volume={20},
  number={2},
  pages={801--824},
  year={1992},
  publisher={Institute of Mathematical Statistics}
}


@article{chi1999statistical,
  title={Statistical properties of probabilistic context-free grammars},
  author={Chi, Zhiyi},
  journal={Computational Linguistics},
  volume={25},
  number={1},
  pages={131--160},
  year={1999},
  publisher={MIT Press}
}

@incollection{shieber1985evidence,
  title={Evidence against the context-freeness of natural language},
  author={Shieber, Stuart M},
  booktitle={Philosophy, Language, and Artificial Intelligence},
  pages={79--89},
  year={1985},
  publisher={Springer}
}
