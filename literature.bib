@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2005},
  author={Mitzenmacher, Michael and Upfal, Eli},
  journal={Cambridge UP}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of LSTMs to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of LSTM Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}


@article{merrill2019sequential,
  title={Sequential neural networks as automata},
  author={Merrill, William},
  journal={arXiv preprint arXiv:1906.01615},
  year={2019}
}



@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}


@article{perez2019turing,
  title={On the Turing Completeness of Modern Neural Network Architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}



@article{chen2017recurrent,
  title={Recurrent neural networks as weighted language recognizers},
  author={Chen, Yining and Gilroy, Sorcha and Maletti, Andreas and May, Jonathan and Knight, Kevin},
  journal={arXiv preprint arXiv:1711.05408},
  year={2017}
}



@incollection{siegelman1991neural,
  title={Neural networks are universal computing devices},
  author={Siegelman, H and Sontag, ED},
  booktitle={Technical Report SYCON-91-08},
  year={1991},
  publisher={Rutgers Center for Systems and Control}
}



@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{tran2018importance,
  title={The importance of being recurrent for modeling hierarchical structure},
  author={Tran, Ke and Bisazza, Arianna and Monz, Christof},
  journal={arXiv preprint arXiv:1803.03585},
  year={2018}
}

@article{yang2019assessing,
  title={Assessing the Ability of Self-Attention Networks to Learn Word Order},
  author={Yang, Baosong and Wang, Longyue and Wong, Derek F and Chao, Lidia S and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1906.00592},
  year={2019}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (sfcs 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10},
keywords={Polynomials;Circuits;Erbium;Frequency selective surfaces;Computer science;Complexity theory;Computational complexity},
doi={10.1109/SFCS.1985.49},
ISSN={0272-5428},
month={Oct},}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in AC0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on SRN behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@article{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  journal={arXiv preprint arXiv:1801.10296},
  year={2018}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for rnn/cnn-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@article{chen2018best,
  title={The best of both worlds: Combining recent advances in neural machine translation},
  author={Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:1804.09849},
  year={2018}
}



@article{hao2019modeling,
  title={Modeling Recurrence for Transformer},
  author={Hao, Jie and Wang, Xing and Yang, Baosong and Wang, Longyue and Zhang, Jinfeng and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:1904.03092},
  year={2019}
}


@article{paulus2017deep,
  title={A deep reinforced model for abstractive summarization},
  author={Paulus, Romain and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1705.04304},
  year={2017}
}


@article{cheng2016long,
  title={Long short-term memory-networks for machine reading},
  author={Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.06733},
  year={2016}
}


@article{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.03130},
  year={2017}
}


@article{parikh2016decomposable,
  title={A decomposable attention model for natural language inference},
  author={Parikh, Ankur P and T{\"a}ckstr{\"o}m, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1606.01933},
  year={2016}
}


